#!/usr/bin/env bash
set -euo pipefail

# ── test framework ────────────────────────────────────────────────────────────
TEST_DIR=""
TESTS_PASSED=0
TESTS_FAILED=0
FAILED_TESTS=()
DUMP2LLM_PATH=""

setup_test_env() {
    TEST_DIR=$(mktemp -d)
    DUMP2LLM_PATH="$(realpath dump2llm)"
    echo "=== Setting up test environment in $TEST_DIR ==="
    
    # Create test directory structure
    mkdir -p "$TEST_DIR"/{subdir1,subdir2/nested,subdir3}
    
    # Create test files with content
    echo "# Python file 1" > "$TEST_DIR/file1.py"
    echo "console.log('test');" > "$TEST_DIR/file1.js"
    echo "# README content" > "$TEST_DIR/README.md"
    echo "plain text" > "$TEST_DIR/plain.txt"
    
    echo "# Python in subdir1" > "$TEST_DIR/subdir1/sub1.py"
    echo "// JS in subdir1" > "$TEST_DIR/subdir1/sub1.js"
    echo "# MD in subdir1" > "$TEST_DIR/subdir1/sub1.md"
    
    echo "# Python in nested" > "$TEST_DIR/subdir2/nested/deep.py"
    echo "// JS in nested" > "$TEST_DIR/subdir2/nested/deep.js"
    
    echo "# Python in subdir3" > "$TEST_DIR/subdir3/sub3.py"
    
    # Create binary file (should be ignored)
    echo -ne '\x00\x01\x02\x03' > "$TEST_DIR/binary.bin"
    
    # Create empty file (should be ignored)
    touch "$TEST_DIR/empty.txt"
    
    cd "$TEST_DIR"
}

cleanup_test_env() {
    [[ -n "$TEST_DIR" ]] && [[ -d "$TEST_DIR" ]] && rm -rf "$TEST_DIR"
}

run_test() {
    local test_name=$1
    local description=$2
    shift 2
    local cmd=("$@")
    
    echo
    echo "--- Test: $test_name ---"
    echo "Description: $description"
    echo "Command: ${cmd[*]}"
    
    local output_file=$(mktemp)
    local error_file=$(mktemp)
    
    local exit_code=0
    "${cmd[@]}" > "$output_file" 2> "$error_file" || exit_code=$?
    
    if [[ $exit_code -eq 0 ]]; then
        echo "✓ PASSED: $test_name"
        TESTS_PASSED=$((TESTS_PASSED + 1))
        
        # Show output summary
        local file_count=$(grep -c "<<< FILE:" "$output_file" || echo "0")
        echo "  → Found $file_count files in output"
        
        rm -f "$output_file" "$error_file"
        return 0
    else
        echo "✗ FAILED: $test_name"
        TESTS_FAILED=$((TESTS_FAILED + 1))
        FAILED_TESTS+=("$test_name")
        
        echo "  → Exit code: $exit_code"
        echo "  → Error output:"
        cat "$error_file" | sed 's/^/    /'
        
        rm -f "$output_file" "$error_file"
        return 1
    fi
}

validate_output() {
    local test_name=$1
    local expected_files=$2
    shift 2
    local cmd=("$@")
    
    local output_file=$(mktemp)
    "${cmd[@]}" > "$output_file" 2>/dev/null
    
    local actual_count=$(grep -c "<<< FILE:" "$output_file" || echo "0")
    
    if [[ "$actual_count" -eq "$expected_files" ]]; then
        echo "✓ VALIDATION PASSED: $test_name ($actual_count files)"
        rm -f "$output_file"
        return 0
    else
        echo "✗ VALIDATION FAILED: $test_name (expected $expected_files, got $actual_count)"
        echo "  → Files found:"
        grep "<<< FILE:" "$output_file" | sed 's/^/    /' || echo "    (none)"
        rm -f "$output_file"
        return 1
    fi
}

# ── test scenarios ────────────────────────────────────────────────────────────

test_single_file() {
    echo
    echo "=== Testing Single File Input ==="
    
    run_test "single_file_py" "Single Python file" \
        "$DUMP2LLM_PATH" file1.py
    
    run_test "single_file_js" "Single JavaScript file" \
        "$DUMP2LLM_PATH" file1.js
    
    validate_output "single_file_validation" 1 \
        $DUMP2LLM_PATH file1.py
}

test_multiple_files() {
    echo
    echo "=== Testing Multiple Files Input ==="
    
    run_test "multiple_files" "Multiple specific files" \
        $DUMP2LLM_PATH file1.py file1.js README.md
    
    validate_output "multiple_files_validation" 3 \
        $DUMP2LLM_PATH file1.py file1.js README.md
}

test_single_glob() {
    echo
    echo "=== Testing Single-Level Glob Patterns ==="
    
    run_test "glob_py_single" "Single-level Python glob" \
        $DUMP2LLM_PATH "*.py"
    
    run_test "glob_js_single" "Single-level JavaScript glob" \
        $DUMP2LLM_PATH "*.js"
    
    run_test "glob_mixed_single" "Single-level mixed glob" \
        $DUMP2LLM_PATH "*.{py,js}"
    
    validate_output "glob_py_single_validation" 1 \
        $DUMP2LLM_PATH "*.py"
}

test_recursive_glob() {
    echo
    echo "=== Testing Recursive Glob Patterns ==="
    
    run_test "glob_py_recursive" "Recursive Python glob" \
        $DUMP2LLM_PATH "**/*.py"
    
    run_test "glob_js_recursive" "Recursive JavaScript glob" \
        $DUMP2LLM_PATH "**/*.js"
    
    validate_output "glob_py_recursive_validation" 4 \
        $DUMP2LLM_PATH "**/*.py"
}

test_directory_input() {
    echo
    echo "=== Testing Directory Input ==="
    
    run_test "directory_current" "Current directory" \
        $DUMP2LLM_PATH .
    
    run_test "directory_subdir1" "Subdirectory input" \
        $DUMP2LLM_PATH subdir1
    
    # Should find text files but not binary/empty
    validate_output "directory_validation" 3 \
        $DUMP2LLM_PATH subdir1
}

test_mixed_inputs() {
    echo
    echo "=== Testing Mixed Input Types ==="
    
    run_test "mixed_file_and_glob" "File + glob pattern" \
        $DUMP2LLM_PATH file1.py "*.js"
    
    run_test "mixed_dir_and_files" "Directory + specific files" \
        $DUMP2LLM_PATH subdir1 file1.py
    
    run_test "mixed_globs" "Single + recursive globs" \
        $DUMP2LLM_PATH "*.md" "**/*.py"
    
    validate_output "mixed_validation" 2 \
        $DUMP2LLM_PATH file1.py "*.js"
}

test_ignore_patterns() {
    echo
    echo "=== Testing Ignore Patterns ==="
    
    run_test "ignore_extension" "Ignore JS files" \
        $DUMP2LLM_PATH --ignore "*.js" "**/*"
    
    run_test "ignore_directory" "Ignore subdir2" \
        $DUMP2LLM_PATH --ignore "subdir2" "**/*"
    
    run_test "ignore_multiple" "Ignore multiple patterns" \
        $DUMP2LLM_PATH --ignore "*.js,subdir2" "**/*"
}

test_edge_cases() {
    echo
    echo "=== Testing Edge Cases ==="
    
    run_test "nonexistent_file" "Non-existent file (should warn)" \
        $DUMP2LLM_PATH nonexistent.py || true
    
    run_test "empty_glob" "Empty glob pattern" \
        $DUMP2LLM_PATH "*.nonexistent" || true
    
    run_test "binary_file" "Binary file (should be ignored)" \
        $DUMP2LLM_PATH binary.bin || true
    
    run_test "empty_file" "Empty file (should be ignored)" \
        $DUMP2LLM_PATH empty.txt || true
}

test_deduplication() {
    echo
    echo "=== Testing Deduplication ==="
    
    run_test "duplicate_inputs" "Same file multiple times" \
        $DUMP2LLM_PATH file1.py file1.py file1.py
    
    run_test "overlapping_patterns" "Overlapping patterns" \
        $DUMP2LLM_PATH "*.py" "**/*.py"
    
    validate_output "deduplication_validation" 1 \
        $DUMP2LLM_PATH file1.py file1.py
}

# ── main test runner ──────────────────────────────────────────────────────────

print_summary() {
    echo
    echo "=========================================="
    echo "TEST SUMMARY"
    echo "=========================================="
    echo "Tests passed: $TESTS_PASSED"
    echo "Tests failed: $TESTS_FAILED"
    echo "Total tests:  $((TESTS_PASSED + TESTS_FAILED))"
    
    if [[ $TESTS_FAILED -gt 0 ]]; then
        echo
        echo "Failed tests:"
        for test in "${FAILED_TESTS[@]}"; do
            echo "  - $test"
        done
        echo
        echo "❌ Some tests failed!"
        exit 1
    else
        echo
        echo "✅ All tests passed!"
        exit 0
    fi
}

# Trap to ensure cleanup
trap cleanup_test_env EXIT

# Main execution
main() {
    echo "Starting dump2llm test suite..."
    
    setup_test_env
    
    test_single_file
    test_multiple_files
    test_single_glob
    test_recursive_glob
    test_directory_input
    test_mixed_inputs
    test_ignore_patterns
    test_edge_cases
    test_deduplication
    
    print_summary
}

main "$@"